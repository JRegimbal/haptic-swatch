{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a94145-25e8-4c1e-aa84-47ffe81a8c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import sys\n",
    "import torch\n",
    "from collections import deque\n",
    "from functools import reduce\n",
    "\n",
    "print(\"===VERSIONS===\")\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"numpy: {np.__version__}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Scipy: {scipy.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c4eb74-b2bc-4bfb-ab48-7e730f5eeaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BasicNN(torch.nn.Module):\n",
    "    def __init__(self, ndim):\n",
    "        super(BasicNN, self).__init__()\n",
    "        hls = round(8*ndim / 3) # chosen by vibes\n",
    "        self.fc1 = torch.nn.Linear(2*ndim, hls)\n",
    "        self.fc2 = torch.nn.Linear(hls, hls)\n",
    "        self.fc3 = torch.nn.Linear(hls, 1)\n",
    "\n",
    "        self.fc1.weight.data.fill_(0)\n",
    "        self.fc2.weight.data.fill_(0)\n",
    "        self.fc3.weight.data.fill_(0)\n",
    "\n",
    "        self.double()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0581cc07-8b06-4f9a-88cb-201da9d0f52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idenfeatures(states, actions):\n",
    "    return np.array([*states, *actions])\n",
    "\n",
    "get_features = idenfeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e6ddc5-91f3-47cc-88b9-88e7615bbb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from scipy.stats import gamma\n",
    "\n",
    "\n",
    "class GammaCrediter:\n",
    "    def __init__(self, ndims):\n",
    "        self._history = []\n",
    "        # From TAMER\n",
    "        self.alpha = 2.0\n",
    "        self.beta = 0.28\n",
    "        self.delay = 0.20 # seconds\n",
    "\n",
    "    def add_index(self, feature_vec):\n",
    "        self._history.append((feature_vec, time.time()))\n",
    "\n",
    "    def credit(self):\n",
    "        # Prune old times\n",
    "        self._history = [x for x in self._history if time.time() - x[1] < gamma.ppf(0.999, self.alpha, self.delay, self.beta)]\n",
    "        self._history.sort(key=lambda x: x[1], reverse=True)\n",
    "        # Calculate from remaining\n",
    "        return sum([\n",
    "            (gamma.cdf(x[1], self.alpha, self.delay, self.beta) - \\\n",
    "             (0 if idx == 0 else gamma.cdf(self._history[idx-1][1], self.alpha, self.delay, self.beta))) * \\\n",
    "            x[0] for idx, x in enumerate(self._history)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8207cf37-3718-4a29-b84e-ee330e00d0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scurto used alpha = 0.002\n",
    "class NeuralSGDAgent:\n",
    "    def __init__(self, ndims, step, epsilon=0.1, alpha=0.02, gamma=0.50, crediter=GammaCrediter):\n",
    "        self._ndims = ndims\n",
    "        self._step = step\n",
    "        self.crediter = crediter(self._ndims)\n",
    "        self.state = np.zeros(self._ndims)\n",
    "        self._net = BasicNN(self._ndims)\n",
    "        self._criterion = torch.nn.MSELoss()\n",
    "        self._optimizer = torch.optim.SGD(self._net.parameters(), lr=alpha)\n",
    "        a = np.eye(self._ndims) * self._step\n",
    "        self._actions = np.concatenate((a, -a))\n",
    "        self._exclude_dims = set()\n",
    "        self._rng = np.random.default_rng()\n",
    "        self._epsilon = epsilon\n",
    "        self._alpha = alpha # taken from scurto et al 2021\n",
    "        self._gamma = gamma\n",
    "\n",
    "    def set_state(self, state, action=None):\n",
    "        if action is None:\n",
    "            action = np.zeros(self._ndims)\n",
    "        self.state = state\n",
    "        self.crediter.add_index(get_features(self.state, action))\n",
    "\n",
    "    def check_bounds(self, state):\n",
    "        return ((state >= 0) & (state <= 1)).all(0)\n",
    "\n",
    "    def get_value(self, state, action):\n",
    "        return self._net(torch.from_numpy(get_features(state, action))).item()\n",
    "\n",
    "    def select_action(self):\n",
    "        max_actions = []\n",
    "        invs = []\n",
    "        max_value = np.NINF\n",
    "        for action in self.included_actions():\n",
    "            next_state = self.state + action\n",
    "            if self.check_bounds(next_state):\n",
    "                value = self.get_value(next_state, action)\n",
    "                if np.isclose(max_value, value):\n",
    "                    max_actions.append(action)\n",
    "                elif value > max_value:\n",
    "                    max_value = value\n",
    "                    max_actions = [action]\n",
    "            else:\n",
    "                invs.append(action)\n",
    "        if len(invs) > 0:\n",
    "            print(f\"Invalid actions {invs}\")\n",
    "        print(f\"Maximum value of {max_value}\")\n",
    "        if len(max_actions) > 0:\n",
    "            return max_actions[self._rng.integers(len(max_actions))]\n",
    "        else:\n",
    "            print(\"No valid actions!\")\n",
    "            return None\n",
    "\n",
    "    def select_epsilon_greedy_action(self):\n",
    "        if self._rng.random() < self._epsilon:   \n",
    "            # Random action\n",
    "            invalid = True\n",
    "            actions = self.included_actions()\n",
    "            if len(actions) > 0:\n",
    "                while invalid:\n",
    "                    action = actions[self._rng.integers(len(actions))]\n",
    "                    next_state = self.state + action\n",
    "                    invalid = not self.check_bounds(next_state)\n",
    "                print(f\"Taking random action {action}\")\n",
    "                return action\n",
    "            else:\n",
    "                print(\"No valid actions!\")\n",
    "                return None\n",
    "        else:\n",
    "            return self.select_action() \n",
    "\n",
    "    def apply_action(self, action):\n",
    "        next_state = self.state + action\n",
    "        if self.check_bounds(next_state):\n",
    "            self.set_state(next_state, action)\n",
    "        else:\n",
    "            raise Exception(f\"Tried to transition to an invalid state {next_state}.\")\n",
    "\n",
    "    def reward_and_bootstrap(self, reward):\n",
    "        credit_features = self.crediter.credit()\n",
    "        action = self.select_action()\n",
    "        if action is not None:\n",
    "            next_state = self.state + action\n",
    "            exp_gain = reward + self._gamma * self.get_value(next_state, action)\n",
    "        else:\n",
    "            exp_gain = reward\n",
    "        self._optimizer.zero_grad()\n",
    "        error = self._criterion(self._net(torch.from_numpy(credit_features)), torch.from_numpy(np.array([exp_gain])))\n",
    "        error.backward()\n",
    "        self._optimizer.step()\n",
    "\n",
    "    def process_zone_reward(self, reward):\n",
    "        # Positive reward - apply towards this point - negative reward - apply away from this point\n",
    "        # Directions include those disabled so we properly encode the zone here\n",
    "        ZONE_STEPS = 3 # Arbitrarily chosen, TODO scale to match number of divisions in a dimension\n",
    "        for action in self._actions:\n",
    "            features = np.zeros(len(get_features(np.zeros(self._ndims), np.zeros(self._ndims))))\n",
    "            for step in range(1, ZONE_STEPS + 1):\n",
    "                state = self.state + step * action\n",
    "                if not self.check_bounds(state):\n",
    "                    break\n",
    "                features += get_features(state, action)\n",
    "            features /= ZONE_STEPS\n",
    "            self._optimizer.zero_grad()\n",
    "            # Use negative reward since we are moving away\n",
    "            error = self._criterion(self._net(torch.from_numpy(features)), torch.from_numpy(np.array([-reward])))\n",
    "            error.backward()\n",
    "            self._optimizer.step()\n",
    "\n",
    "    def update_activation(self, dimension, activation):\n",
    "        if activation:\n",
    "            self._exclude_dims.discard(dimension)\n",
    "        else:\n",
    "            self._exclude_dims.add(dimension)\n",
    "\n",
    "    def included_actions(self):\n",
    "        # Set of actions that do not modify the 0-indexed dimensions in self._exclude_dims\n",
    "        return np.array([act for act in self._actions if reduce(lambda x, y: x and y, [act[dim] == 0 for dim in self._exclude_dims], True)])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb88bc5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pythonosc.dispatcher import Dispatcher\n",
    "from pythonosc.osc_server import ThreadingOSCUDPServer\n",
    "from pythonosc.udp_client import SimpleUDPClient\n",
    "from threading import Thread\n",
    "import time\n",
    "\n",
    "manualMode = True\n",
    "agents = {}\n",
    "\n",
    "ip = \"127.0.0.1\" # localhost\n",
    "port = 8080\n",
    "destPort = 8081\n",
    "\n",
    "client = SimpleUDPClient(ip, destPort)\n",
    "\n",
    "def default_handler(address, *args):\n",
    "    print(f\"DEFAULT {address}: {args}\")\n",
    "\n",
    "def auto_switch_handler(address, state, *args):\n",
    "    print(f\"Is Manual {state}\")\n",
    "    manualMode = state\n",
    "\n",
    "def manual_set(address, element, *args):\n",
    "    # Currently assume only one element - TODO revise later\n",
    "    agents[element].set_state(args)\n",
    "    print(f\"{element}: {agents[element].state}\")\n",
    "\n",
    "def step(address, element):\n",
    "    old_state = agents[element].state\n",
    "    action = agents[element].select_epsilon_greedy_action()\n",
    "    if action is not None:\n",
    "        print(f\"{element}: Taking action {action}\")\n",
    "        agents[element].apply_action(action)\n",
    "        #print(f\"Transitioned from {old_state} to {agent.state}\")\n",
    "        client.send_message(\"/controller/agentSet\", [element, *agents[element].state])\n",
    "    else:\n",
    "        print(f\"{element}: All actions excluded! Doing nothing.\")\n",
    "\n",
    "def reward(address, element, reward):\n",
    "    # Currently assuming only one element - TODO revise later\n",
    "    #old_weights = agents[element]._weights\n",
    "    agents[element].reward_and_bootstrap(reward)\n",
    "    # print(f\"Weights updated from {old_weights} to {agent._weights}\")\n",
    "\n",
    "def zone_reward(address, element, reward):\n",
    "    # Calculate length N_STEPS away on each axis, store in agent\n",
    "    agents[element].process_zone_reward(reward)\n",
    "    \n",
    "\n",
    "def activate(address, element, dimension, activation):\n",
    "    print(f\"{element}: Setting dimension {dimension} to {activation}\")\n",
    "    agents[element].update_activation(dimension, activation)\n",
    "    print(f\"{agents[element]._exclude_dims}\")\n",
    "\n",
    "def init(address, element, ndims, step):\n",
    "    if element in agents:\n",
    "        print(f\"Replacing agent {element} with fresh. {ndims} dimensions, initial step {step} (norm)\")\n",
    "    else:\n",
    "        print(f\"New agent {element} with {ndims} dimensions, initial step {step} (norm)\")\n",
    "    #agents[element] = LinearSGDAgent(ndims, step)\n",
    "    agents[element] = NeuralSGDAgent(ndims, step)\n",
    "\n",
    "def delete(address, element):\n",
    "    if element in agents:\n",
    "        print(f\"Deleting agent {element} ({agents[element]._ndims} dimensions)\")\n",
    "        del agents[element]\n",
    "    else:\n",
    "        print(f\"No agent with identifier {element}!\")\n",
    "\n",
    "\n",
    "dispatcher = Dispatcher()\n",
    "dispatcher.set_default_handler(default_handler)\n",
    "dispatcher.map(\"/uistate/setAutonomous\", auto_switch_handler)\n",
    "dispatcher.map(\"/controller/manualSet\", manual_set)\n",
    "dispatcher.map(\"/controller/step\", step)\n",
    "dispatcher.map(\"/controller/reward\", reward)\n",
    "dispatcher.map(\"/controller/activate\", activate)\n",
    "dispatcher.map(\"/controller/init\", init)\n",
    "dispatcher.map(\"/controller/zone_reward\", zone_reward)\n",
    "\n",
    "ip = \"127.0.0.1\" # localhost\n",
    "port = 8080\n",
    "\n",
    "with ThreadingOSCUDPServer((ip, port), dispatcher) as server:\n",
    "    def quit_func(address, *args):\n",
    "        print(\"Quit!\")\n",
    "        server.shutdown()\n",
    "        server.server_close()\n",
    "    dispatcher.map(\"/quit\", quit_func)\n",
    "    thread = Thread(target=server.serve_forever)\n",
    "    thread.start()\n",
    "    thread.join()\n",
    "print(\"And we're out!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320cf03b-a003-4130-b9a7-abda5474cf58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
