{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a94145-25e8-4c1e-aa84-47ffe81a8c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from collections import deque\n",
    "from functools import reduce\n",
    "\n",
    "print(\"===VERSIONS===\")\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"numpy: {np.__version__}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Scipy: {scipy.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02c4b40-27e5-47de-a609-16cbc33f2eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAM_EPSILON = 0.1\n",
    "PARAM_ALPHA = 0.002\n",
    "PARAM_BETA = 1\n",
    "PARAM_C = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5480add-9368-49a5-ab53-98b6ebfd9f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizedTiling:\n",
    "    def __init__(self, ndim, ksteps, offset):\n",
    "        self.ndim = ndim\n",
    "        self.k = ksteps\n",
    "        self.offset = offset\n",
    "        assert self.offset < 0, \"Offset must be less than 0\"\n",
    "        assert self.offset >= -1 / self.k, f\"Offset cannot be above {-1/self.k}\"\n",
    "        self.counts = np.zeros(np.power(self.k + 1, self.ndim))\n",
    "\n",
    "    def tile_index(self, state):\n",
    "        assert len(state) == self.ndim, f\"Expected state of dimension {self.ndim}, received {len(state)}\"\n",
    "        shift = (state - self.offset * np.ones(self.ndim)) * self.k\n",
    "        return sum([int(np.power(self.k + 1, j) * (np.floor(shift[j]) if shift[j] < self.k else self.k)) for j in range(self.ndim)])\n",
    "\n",
    "class TilingDensity:\n",
    "    def __init__(self, ndim, ntiles, ksteps):\n",
    "        self.tiles = [NormalizedTiling(ndim, ksteps, -(i + 1)/(ksteps * ntiles)) for i in range(ntiles)]\n",
    "        self.total_count = 0\n",
    "\n",
    "    def count(self, x):\n",
    "        for tiling in self.tiles:\n",
    "            idx = tiling.tile_index(x)\n",
    "            tiling.counts[idx] += 1\n",
    "        self.total_count += 1\n",
    "\n",
    "    def density(self, x):\n",
    "        if self.total_count > 0:\n",
    "            return sum([tiling.counts[tiling.tile_index(x)] for tiling in self.tiles]) / self.total_count\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c4eb74-b2bc-4bfb-ab48-7e730f5eeaea",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class BasicNN(torch.nn.Module):\n",
    "    def __init__(self, ndim):\n",
    "        super(BasicNN, self).__init__()\n",
    "        hls = round(8*ndim / 3) # chosen by vibes\n",
    "        #hls = 10 * 2 * ndim\n",
    "        self.fc1 = torch.nn.Linear(2*ndim, hls)\n",
    "        self.fc2 = torch.nn.Linear(hls, hls)\n",
    "        self.fc3 = torch.nn.Linear(hls, 1)\n",
    "\n",
    "        torch.nn.init.normal_(self.fc1.weight, std=0.1)\n",
    "        torch.nn.init.normal_(self.fc1.weight, std=0.1)\n",
    "        torch.nn.init.normal_(self.fc3.weight, std=0.1)\n",
    "\n",
    "        #torch.nn.init.zeros_(self.fc1.bias)\n",
    "        #torch.nn.init.zeros_(self.fc2.bias)\n",
    "        #torch.nn.init.zeros_(self.fc3.bias)\n",
    "\n",
    "        self.double()\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = F.relu(self.fc1(x))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        #x = self.fc3(x)\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        x = F.tanh(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392e4b73-6e2c-42b8-a177-e17d01efffe5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class DeepNN(torch.nn.Module):\n",
    "    def __init__(self, ndim):\n",
    "        super(DeepNN, self).__init__()\n",
    "        hls = 10 * 2 * ndim\n",
    "        self.fc1 = torch.nn.Linear(2* ndim, hls)\n",
    "        self.fc2 = torch.nn.Linear(hls, hls)\n",
    "        self.fc3 = torch.nn.Linear(hls, hls)\n",
    "        self.fc4 = torch.nn.Linear(hls, 1)\n",
    "        \n",
    "        torch.nn.init.normal_(self.fc1.weight, std=0.1)\n",
    "        torch.nn.init.normal_(self.fc2.weight, std=0.1)\n",
    "        torch.nn.init.normal_(self.fc3.weight, std=0.1)\n",
    "        torch.nn.init.normal_(self.fc4.weight, std=0.1)\n",
    "\n",
    "        self.double()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        x = F.tanh(self.fc3(x))\n",
    "        x = F.tanh(self.fc4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c3f782-0b7c-4b80-8890-013cfc990494",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLUNet(torch.nn.Module):\n",
    "    def __init__(self, ndim):\n",
    "        super(ReLUNet, self).__init__()\n",
    "        hls = 100\n",
    "        self.fc1 = torch.nn.Linear(ndim, hls)\n",
    "        self.fc2 = torch.nn.Linear(hls, hls)\n",
    "        self.fc3 = torch.nn.Linear(hls, 2 * ndim)\n",
    "        self.action_size = 2 * ndim\n",
    "\n",
    "        torch.nn.init.normal_(self.fc1.weight, std=0.1)\n",
    "        torch.nn.init.normal_(self.fc1.weight, std=0.1)\n",
    "        torch.nn.init.normal_(self.fc3.weight, std=0.1)\n",
    "        \n",
    "        self.double()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sub(x, 0.5) # TODO does this help/hurt?\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688f7b58-0c1e-4289-8a20-31c685b2552d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTAMERLoss(torch.nn.Module):\n",
    "    def __init__(self, actionsize):\n",
    "        super(DTAMERLoss, self).__init__()\n",
    "        self.actionsize = actionsize\n",
    "\n",
    "    def forward(self, result, target, action, weights=None):\n",
    "        self.one_hot = F.one_hot(action, num_classes=self.actionsize).to(torch.float64)\n",
    "        self.q = torch.mul(self.one_hot, result).sum(axis=1)\n",
    "        if weights is not None:\n",
    "            self.error = (weights * torch.square(target - self.q)).mean()\n",
    "        else:\n",
    "            self.error = ((target - self.q) ** 2).mean()\n",
    "        return self.error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0581cc07-8b06-4f9a-88cb-201da9d0f52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def idenfeatures(states, actions):\n",
    "#    return np.array([*states, *actions])\n",
    "\n",
    "#def idenstate(states, actions):\n",
    "#    return np.array([*states])\n",
    "\n",
    "#get_features = idenstate\n",
    "\n",
    "def and_op(x, y):\n",
    "    return x and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e6ddc5-91f3-47cc-88b9-88e7615bbb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from scipy.stats import gamma\n",
    "\n",
    "\n",
    "class LinearGammaCrediter:\n",
    "    def __init__(self, ndims):\n",
    "        self._history = []\n",
    "        # From TAMER\n",
    "        self.k = 2.0\n",
    "        self.theta = 0.28\n",
    "        self.delay = 0.20 # seconds\n",
    "\n",
    "    def add_index(self, feature_vec):\n",
    "        self._history.append((feature_vec, time.time()))\n",
    "\n",
    "    def credit(self):\n",
    "        # Prune old times\n",
    "        self._history = [x for x in self._history if time.time() - x[1] < gamma.ppf(0.999, self.k, self.delay, self.theta)]\n",
    "        self._history.sort(key=lambda x: x[1], reverse=True)\n",
    "        if len(self._history) == 0:\n",
    "            raise Exception(\"Empty history array - cannot assign credit\")\n",
    "        # Calculate from remaining\n",
    "        return sum([\n",
    "            (gamma.cdf(x[1], self.k, self.delay, self.theta) - \\\n",
    "             (0 if idx == 0 else gamma.cdf(self._history[idx-1][1], self.k, self.delay, self.theta))) * \\\n",
    "            x[0] for idx, x in enumerate(self._history)\n",
    "        ])\n",
    "\n",
    "class GammaCrediter:\n",
    "    \"\"\"\n",
    "    Unlike the above/older version of the crediter, this version\n",
    "    does not combine the history into one feature vector. Instead,\n",
    "    each vector is returned alongside a weight. Weights sum to 1.\n",
    "    \"\"\"\n",
    "    def __init__(self, ndims):\n",
    "        self._history = []\n",
    "        self.k = 2.0\n",
    "        self.theta = 0.28\n",
    "        self.delay = 0.20\n",
    "        \n",
    "    def add_index(self, feature_vec):\n",
    "        self._history.append((feature_vec, time.time()))\n",
    "    \n",
    "    def credit(self):\n",
    "        # Prune old times\n",
    "        self._history = [x for x in self._history if time.time() - x[1] < gamma.ppf(0.999, self.k, self.delay, self.theta)]\n",
    "        self._history.sort(key=lambda x: x[1], reverse=True)\n",
    "        if len(self._history) == 0:\n",
    "            raise Exception(\"Empty history array - cannot assign credit\")\n",
    "        current_time = time.time()\n",
    "        weights = np.array([gamma.cdf(current_time - x[1], self.k, self.delay, self.theta) - \\\n",
    "                  (0 if idx == 0 else gamma.cdf(current_time - self._history[idx-1][1], self.k, self.delay, self.theta)) \\\n",
    "                  for idx, x in enumerate(self._history)]).reshape((len(self._history), 1))\n",
    "        return (np.array([x[0] for x in self._history]), weights)\n",
    "\n",
    "    def credit2(self):\n",
    "        # Prune old times\n",
    "        next = []\n",
    "        self._history = [x for x in self._history if time.time() - x[1] < gamma.ppf(0.999, self.k, self.delay, self.theta)]\n",
    "        self._history.sort(key=lambda x: x[1], reverse=True)\n",
    "        if len(self._history) == 0:\n",
    "            raise Exception(\"Empty history array - cannot assign credit\")\n",
    "        current_time = time.time()\n",
    "        weights = np.array([gamma.cdf(current_time - x[1], self.k, self.delay, self.theta) - \\\n",
    "                  (0 if idx == 0 else gamma.cdf(current_time - self._history[idx-1][1], self.k, self.delay, self.theta)) \\\n",
    "                  for idx, x in enumerate(self._history)]).reshape((len(self._history), 1))\n",
    "        return (np.array([x[0] for x in self._history]), weights, np.zeros(len(weights)))\n",
    "\n",
    "class UniformCrediter:\n",
    "    \"\"\"\n",
    "    Equally splits reward over interval of t-0.2 to t-4. Weights sum to 1\n",
    "    \"\"\"\n",
    "    def __init__(self, ndims):\n",
    "        self._history = []\n",
    "        self._low = 0.1\n",
    "        self._high = 2.0\n",
    "\n",
    "    def add_index(self, feature_vec):\n",
    "        self._history.append((feature_vec, time.time()))\n",
    "\n",
    "    def credit(self):\n",
    "        # Prune old times\n",
    "        call_time = time.time()\n",
    "        self._history = [x for x in self._history if call_time - x[1] <= self._high]\n",
    "        tmp = [x for x in self._history if call_time - x[1] >= self._low]\n",
    "        if len(tmp) == 0:\n",
    "            raise Exception(\"No eligible history - cannot assign credit\")\n",
    "        return (np.array([x[0] for x in tmp]), np.ones((len(tmp), 1)) / len(tmp)) \n",
    "\n",
    "    def credit2(self):\n",
    "        call_time = time.time()\n",
    "        self._history = [x for x in self._history if call_time - x[1] <= self._high] # Limit to those eligible to receive credit\n",
    "        values = []\n",
    "        next = []\n",
    "        for i in range(len(self._history)):\n",
    "            if call_time - self._history[i][1] >= self._low:\n",
    "                values.append(self._history[i][0])\n",
    "                if i < len(self._history) - 1:\n",
    "                    next.append(self._history[i + 1][0])\n",
    "                else:\n",
    "                    # This case shouldn't happen\n",
    "                    print(\"Entire history eligible for credit, this shouldn't typically happen!\")\n",
    "                    next.append(self._history[i][0]) # Can't give dummy value, assuming that the person wants a no-op as much as possible\n",
    "            else:\n",
    "                break\n",
    "        assert len(values) == len(next), f\"{len(values)} values eligible, but {len(next)} next state-actions found\"\n",
    "        if len(values) == 0:\n",
    "            raise Exception(\"No eligible history - cannot assign credit\")\n",
    "        weights = np.ones((len(values), 1)) # / len(values)\n",
    "        return (np.array(values), weights, np.array(next))\n",
    "\n",
    "    def credit3(self):\n",
    "        call_time = time.time()\n",
    "        self._history = [x for x in self._history if call_time - x[1] <= self._high]\n",
    "        values = [i[0] for i in self._history if call_time - i[1] > self._low]\n",
    "        states = np.array([i[0] for i in values])\n",
    "        actions = np.array([[i[1]] for i in values])\n",
    "        return (states, np.ones(actions.shape), actions)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df35f1aa-3ef9-4ceb-b0f5-e9cd8a147e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAgent:\n",
    "    def __init__(self, ndims, step, *args):\n",
    "        self._ndims = ndims\n",
    "        self._step = step\n",
    "        self.state = np.zeros(self._ndims)\n",
    "        self.state_lows = np.zeros(self._ndims)\n",
    "        self.state_highs = np.ones(self._ndims)\n",
    "        a = np.eye(self._ndims) * self._step\n",
    "        self._actions = np.concatenate((a, -a))\n",
    "        self._actions_index = list(range(len(self._actions)))\n",
    "        self._exclude_dims = set()\n",
    "        self._rng = np.random.default_rng()\n",
    "\n",
    "# Values should be normalized to 0-1 space for each\n",
    "    def set_state(self, state, *, lows=None, highs=None, action=None, history=True):\n",
    "        state = np.array(state)\n",
    "        # Check range and update\n",
    "        if lows is not None:\n",
    "            assert len(lows) == self._ndims, f\"Expected lows to contain {self._ndims} elements, not {len(lows)}\"\n",
    "            assert reduce(and_op, [x >= 0 for x in lows], True) and reduce(and_op, [x <= 1 for x in lows], True), f\"Low range not normalized: {lows}\"\n",
    "            self.state_lows = lows\n",
    "        if highs is not None:\n",
    "            assert len(highs) == self._ndims, f\"Expected highs to contain {self._ndims} elements, not {len(highs)}\"\n",
    "            assert reduce(and_op, [x >= 0 for x in highs], True) and reduce(and_op, [x <= 1 for x in highs], True), f\"High range not normalized: {highs}\"\n",
    "            self.state_highs = highs\n",
    "        assert reduce(and_op, [x >= 0 for x in state], True) and reduce(and_op, [x <= 1 for x in state], True), f\"State out of bounds {state}\"\n",
    "        old_state = self.state\n",
    "        self.state = state\n",
    "        return old_state\n",
    "\n",
    "    def to_action(self, action):\n",
    "        return self._actions[action]\n",
    "\n",
    "    def _check_bounds(self, state):\n",
    "        return ((state >= 0) & (state <= 1) & (state >= np.array(self.state_lows)) & (state <= np.array(self.state_highs))).all(0)\n",
    "\n",
    "    def apply_action(self, action):\n",
    "        next_state = self.state + self.to_action(action)\n",
    "        if self._check_bounds(next_state):\n",
    "            self.set_state(next_state, action=action)\n",
    "        else:\n",
    "            raise Exception(f\"Tried to transition to an invalid state {next_state}.\")\n",
    "\n",
    "    def update_activation(self, dimension, activation):\n",
    "        if activation:\n",
    "            self._exclude_dims.discard(dimension)\n",
    "        else:\n",
    "            self._exclude_dims.add(dimension)\n",
    "\n",
    "    def _included_actions(self):\n",
    "        return np.array([act for act in self._actions_index if reduce(lambda x, y: x and y, [self.to_action(act)[dim] == 0 for dim in self._exclude_dims], True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8207cf37-3718-4a29-b84e-ee330e00d0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "# Scurto used alpha = 0.002\n",
    "class NeuralSGDAgent(BaseAgent):\n",
    "    def __init__(self, ndims, step, epsilon=PARAM_EPSILON, alpha=PARAM_ALPHA, crediter=UniformCrediter, replay=True):\n",
    "        BaseAgent.__init__(self, ndims, step)\n",
    "        self.crediter = crediter(self._ndims)\n",
    "        self._net = ReLUNet(self._ndims)\n",
    "        #self._criterion = torch.nn.MSELoss()\n",
    "        self._criterion = DTAMERLoss(2 * self._ndims)\n",
    "        self._optimizer = torch.optim.SGD(self._net.parameters(), lr=alpha)\n",
    "        self._epsilon = epsilon\n",
    "        self._alpha = alpha\n",
    "        self._beta = PARAM_BETA\n",
    "        self._c = PARAM_C\n",
    "        n_tiles = int(2 + np.ceil(np.log2(self._ndims)))\n",
    "        k_tile = int(np.ceil(1 / (4 * self._step)))\n",
    "        #n_tiles = int(2 + np.ceil(np.log2(self._ndims)))\n",
    "        #k_tile = int(np.ceil(1/(2 * self._step)))\n",
    "        self.tiling = TilingDensity(self._ndims, n_tiles, k_tile)\n",
    "        self.replay = replay\n",
    "        self._replay_batch = 32\n",
    "        self._history = deque(maxlen=700)\n",
    "\n",
    "    def set_state(self, state, *, lows=None, highs=None, action=None, history=True):\n",
    "        # Check range and update\n",
    "        old_state = BaseAgent.set_state(self, state, lows=lows, highs=highs, action=action, history=history)\n",
    "        if history:\n",
    "            self.tiling.count(old_state)\n",
    "            if action is not None:\n",
    "                self.crediter.add_index((old_state, action))\n",
    "\n",
    "    def _select_action(self):\n",
    "        start_time = time.time()\n",
    "        max_actions = []\n",
    "        invs = []\n",
    "        max_value = np.NINF\n",
    "        valid_actions = [action for action in self._included_actions() if self._check_bounds(self.state + self.to_action(action))]\n",
    "        if len(valid_actions) > 0:\n",
    "            action_values = self._net(torch.from_numpy(self.state)).detach().numpy()[valid_actions]\n",
    "            explore_values = np.array([self._beta * np.power(\n",
    "                self.tiling.density(self.state + self.to_action(action)) * self.tiling.total_count + self._c,\n",
    "                -0.5\n",
    "            ) for action in valid_actions])\n",
    "            valid_values = action_values + explore_values\n",
    "            #print(f\"Action values ({len(action_values)}): {action_values}\")\n",
    "            #print(f\"Explore values ({len(explore_values)}): {explore_values}\")\n",
    "            max_ind = np.argmax(valid_values)\n",
    "            #print(f\"Selected {max_ind} of {len(valid_values)} values\")\n",
    "            #print(f\"Reward: [{np.min(action_values)}, {np.max(action_values)}], Explore: [{np.min(explore_values)}, {np.max(explore_values)}]\")\n",
    "            end_time = time.time()\n",
    "            timings.append({\"key\": \"step_greedy\", \"start\": start_time, \"end\": end_time})\n",
    "            return valid_actions[max_ind]\n",
    "        else:\n",
    "            print(\"No valid actions!\")\n",
    "            return None\n",
    "\n",
    "    def select_epsilon_greedy_action(self):\n",
    "        if self._rng.random() < self._epsilon:\n",
    "            start_time = time.time()\n",
    "            # Exploration-only action\n",
    "            valid_actions = [action for action in self._included_actions() if self._check_bounds(self.state + self.to_action(action))]\n",
    "            if len(valid_actions) > 0:\n",
    "                feature_explore_values = np.array([self._beta * np.power(self.tiling.density(self.state + self.to_action(act)) * self.tiling.total_count + self._c, -0.5) for act in valid_actions])\n",
    "                max_ind = np.argmax(feature_explore_values)\n",
    "                #max_ind = self._rng.integers(len(valid_actions))\n",
    "                end_time = time.time()\n",
    "                timings.append({\"key\": \"step_rand\", \"start\": start_time, \"end\": end_time})\n",
    "                return valid_actions[max_ind]\n",
    "            else:\n",
    "                print(\"No valid actions!\")\n",
    "                return None\n",
    "        else:\n",
    "            return self._select_action()\n",
    "\n",
    "    def replay_from_history(self):\n",
    "        global losses\n",
    "        if len(self._history) >= 2 * self._replay_batch:\n",
    "            sample = random.sample(self._history, self._replay_batch)\n",
    "            states = np.array([x[0] for x in sample])\n",
    "            weights = np.array([x[1] for x in sample])\n",
    "            actions = np.array([x[2] for x in sample])\n",
    "            self._optimizer.zero_grad()\n",
    "            error = self._criterion(self._net(torch.from_numpy(states)), torch.from_numpy(weights), torch.from_numpy(actions))\n",
    "            error.backward()\n",
    "            self._optimizer.step()\n",
    "            losses.append((time.time(), error.detach().numpy()))\n",
    "            print(\"Replayed from history\")\n",
    "\n",
    "    def process_guiding_reward(self, reward):\n",
    "        global losses\n",
    "        try:\n",
    "            states, credit_weight, actions = self.crediter.credit3()\n",
    "            print(states.shape)\n",
    "            credit_weight = credit_weight * reward\n",
    "            # credit_x := credit_x + gamma * q(snext, anext, weights)\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: {e}\")\n",
    "            print(\"Not applying reward...\")\n",
    "            return\n",
    "        print(\"determined guidance\")\n",
    "        self._optimizer.zero_grad()\n",
    "        error = self._criterion(self._net(torch.from_numpy(states)), torch.from_numpy(credit_weight), torch.from_numpy(actions))\n",
    "        print(f\"Error: {error}\")\n",
    "        error.backward()\n",
    "        self._optimizer.step()\n",
    "\n",
    "        losses.append((time.time(), error.detach().numpy()))\n",
    "\n",
    "        print(\"updated model\")\n",
    "        if self.replay:\n",
    "            history_buf = list(zip(states, credit_weight, actions))\n",
    "            self._history.extend(history_buf)\n",
    "\n",
    "    def process_zone_reward(self, reward):\n",
    "        # Positive reward - apply towards this point - negative reward - apply away from this point\n",
    "        # Directions include those disabled so we properly encode the zone here\n",
    "        SCALE_FACTOR = 1\n",
    "        ZONE_STEPS = 3 # Arbitrarily chosen, TODO scale to match number of divisions in a dimension. Note that Scurto et al. effectively used 5.\n",
    "        features = []\n",
    "        weights = []\n",
    "        reward_mag = reward #/ (ZONE_STEPS * len(self._actions))\n",
    "        for action in self._actions:\n",
    "            state = self.state\n",
    "            # Iterate through steps and apply 1) reward in direction we want to move 2) -reward in direction we do not want to move\n",
    "            for _step in range(1, ZONE_STEPS + 1):\n",
    "                tmp = state + action\n",
    "                if not self._check_bounds(tmp):\n",
    "                    break\n",
    "                features.append(get_features(tmp, action))\n",
    "                weights.append(np.array([-float(SCALE_FACTOR * reward_mag)]))\n",
    "                features.append(get_features(state, -action))\n",
    "                weights.append(np.array([float(SCALE_FACTOR * reward_mag)]))\n",
    "                \n",
    "                state = tmp\n",
    "        self._optimizer.zero_grad()\n",
    "        error = self._criterion(self._net(torch.from_numpy(np.array(features))), torch.from_numpy(np.array(weights)))\n",
    "        print(f\"Error: {error}\")\n",
    "        error.backward()\n",
    "        self._optimizer.step()\n",
    "        if self.replay:\n",
    "            history_buf = list(zip(features, weights))\n",
    "            self._history.extend(history_buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3339266-cc19-4477-bc94-850a13fe03fb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class SplitNeuralSGDAgent(NeuralSGDAgent):\n",
    "    def __init__(self, ndims1, ndims2, step, epsilon=PARAM_EPSILON, alpha=PARAM_ALPHA, crediter=UniformCrediter, gamma=PARAM_GAMMA, replay=True):\n",
    "        NeuralSGDAgent.__init__(self, ndims1 + ndims2, step, epsilon, alpha, crediter, gamma, replay)\n",
    "        self._split_index = ndims1\n",
    "        self.crediter = None\n",
    "        self.crediter1 = crediter(ndims1)\n",
    "        self.crediter2 = crediter(ndims2)\n",
    "        self._net = None\n",
    "        self._net1 = DeepNN(ndims1)\n",
    "        self._net2 = DeepNN(ndims2)\n",
    "        #self._net1 = BasicNN(ndims1)\n",
    "        #self._net2 = BasicNN(ndims2)\n",
    "        self._optimizer = None\n",
    "        self._optimizer1 = torch.optim.SGD(self._net1.parameters(), lr=alpha)\n",
    "        self._optimizer2 = torch.optim.SGD(self._net2.parameters(), lr=alpha)\n",
    "\n",
    "    def split(self, value):\n",
    "        assert len(value) == self._ndims, f\"Expected vector of size {self._ndims}, received {len(value)}\"\n",
    "        return (value[:self._split_index], value[self._split_index:])\n",
    "\n",
    "    def set_state(self, state, *, lows=None, highs=None, action=None, history=True):\n",
    "        old_state = BaseAgent.set_state(self, state, lows=lows, highs=highs, action=action, history=history)\n",
    "        if history:\n",
    "            self.tiling.count(old_state)\n",
    "            if action is not None:\n",
    "                state1, state2 = self.split(old_state)\n",
    "                action1, action2 = self.split(action)\n",
    "                self.crediter1.add_index(get_features(state1, action1))\n",
    "                self.crediter2.add_index(get_features(state2, action2))\n",
    "\n",
    "    def _get_value(self, state, action):\n",
    "        state1, state2 = self.split(state)\n",
    "        action1, action2 = self.split(action)\n",
    "        return self._net1(torch.from_numpy(get_features(state1, action1))).item() + self._net2(torch.from_numpy(get_features(state2, action2))).item()\n",
    "\n",
    "    def _select_action(self):\n",
    "        start_time = time.time()\n",
    "        valid_actions = [action for action in self._included_actions() if self._check_bounds(self.state + action)]\n",
    "        if len(valid_actions) > 0:\n",
    "            features1 = np.array([get_features(self.state[:self._split_index] + action[:self._split_index], action[:self._split_index]) for action in valid_actions])\n",
    "            features2 = np.array([get_features(self.state[self._split_index:] + action[self._split_index:], action[self._split_index:]) for action in valid_actions])\n",
    "            feature_reward_values = self._net1(torch.from_numpy(features1)).detach().numpy() + self._net2(torch.from_numpy(features2)).detach().numpy()\n",
    "            feature_explore_values = np.array([[self._beta * np.power(self.tiling.density(self.state + action) * self.tiling.total_count + self._c, -0.5)] for action in valid_actions])\n",
    "            feature_values = feature_reward_values + feature_explore_values\n",
    "            max_ind = np.argmax(feature_values)\n",
    "            end_time = time.time()\n",
    "            timings.append({\"key\": \"step_greedy\", \"start\": start_time, \"end\": end_time})\n",
    "            return valid_actions[max_ind]\n",
    "        else:\n",
    "            print(\"No valid actions!\")\n",
    "            return None\n",
    "\n",
    "    def replay_from_history(self):\n",
    "        if len(self._history) >= 2 * self._replay_batch:\n",
    "            sample = random.sample(self._history, self._replay_batch)\n",
    "            a1 = np.array([x[0] for x in sample if x[2] == 1])\n",
    "            a2 = np.array([x[0] for x in sample if x[2] == 2])\n",
    "            b1 = np.array([x[1] for x in sample if x[2] == 1])\n",
    "            b2 = np.array([x[1] for x in sample if x[2] == 2])\n",
    "            print(a2.shape)\n",
    "            print(b2.shape)\n",
    "            if len(a1) > 0:\n",
    "                self._optimizer1.zero_grad()\n",
    "                error = self._criterion(self._net1(torch.from_numpy(a1)), torch.from_numpy(b1))\n",
    "                error.backward()\n",
    "                self._optimizer1.step()\n",
    "            if len(a2) > 0:\n",
    "                self._optimizer2.zero_grad()\n",
    "                error = self._criterion(self._net2(torch.from_numpy(a2)), torch.from_numpy(b2))\n",
    "                error.backward()\n",
    "                self._optimizer2.step()\n",
    "            print(\"Replayed from history\")\n",
    "\n",
    "    def process_guiding_reward(self, reward, modality):\n",
    "        assert modality is not None, \"No modality specified\"\n",
    "        try:\n",
    "            credit_x, credit_y, next = self.crediter1.credit2() if modality == 1 else self.crediter2.credit2()\n",
    "            credit_y = credit_y * reward\n",
    "            # credit_x := credit_x + gamma * q(snext, anext, weights)\n",
    "            if self._gamma > 0:\n",
    "                start = time.time()\n",
    "                if modality == 1:\n",
    "                    credit_x = credit_x + self._gamma * self._net1(torch.from_numpy(next)).detach().numpy()\n",
    "                else:\n",
    "                    credit_x = credit_x + self._gamma * self._net2(torch.from_numpy(next)).detach().numpy()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: {e}\")\n",
    "            print(\"Not applying reward...\")\n",
    "            return\n",
    "        if modality == 1:\n",
    "            self._optimizer1.zero_grad()\n",
    "            error = self._criterion(self._net1(torch.from_numpy(credit_x)), torch.from_numpy(credit_y))\n",
    "            print(f\"Error: {error}\")\n",
    "            error.backward()\n",
    "            self._optimizer1.step()\n",
    "        else:\n",
    "            self._optimizer2.zero_grad()\n",
    "            error = self._criterion(self._net2(torch.from_numpy(credit_x)), torch.from_numpy(credit_y))\n",
    "            print(f\"Error: {error}\")\n",
    "            error.backward()\n",
    "            self._optimizer2.step()\n",
    "\n",
    "        if self.replay:\n",
    "            history_buf = list(zip(credit_x, credit_y, np.ones(credit_y.shape) * modality))\n",
    "            self._history.extend(history_buf)\n",
    "\n",
    "    def process_zone_reward(self, reward):\n",
    "        # Positive reward - apply towards this point - negative reward - apply away from this point\n",
    "        # Directions include those disabled so we properly encode the zone here\n",
    "        SCALE_FACTOR = 1\n",
    "        ZONE_STEPS = 5 # Arbitrarily chosen, TODO scale to match number of divisions in a dimension. Note that Scurto et al. effectively used 5.\n",
    "        features1 = []\n",
    "        features2 = []\n",
    "        weights = []\n",
    "        for action in self._actions:\n",
    "            state = self.state\n",
    "            # Iterate through steps and apply 1) reward in direction we want to move 2) -reward in direction we do not want to move\n",
    "            for _step in range(1, ZONE_STEPS + 1):\n",
    "                tmp = state + action\n",
    "                if not self._check_bounds(tmp):\n",
    "                    break\n",
    "                tmp1, tmp2 = self.split(tmp)\n",
    "                state1, state2 = self.split(state)\n",
    "                action1, action2 = self.split(action)\n",
    "                features1.append(get_features(tmp1, action1))\n",
    "                features2.append(get_features(tmp2, action2))\n",
    "                weights.append(np.array([-float(SCALE_FACTOR * reward)]))\n",
    "                features1.append(get_features(state1, -action1))\n",
    "                features2.append(get_features(state2, -action2))\n",
    "                weights.append(np.array([float(SCALE_FACTOR * reward)]))\n",
    "                \n",
    "                state = tmp\n",
    "        self._optimizer1.zero_grad()\n",
    "        error = self._criterion(self._net1(torch.from_numpy(np.array(features1))), torch.from_numpy(np.array(weights)))\n",
    "        print(f\"Error: {error}\")\n",
    "        error.backward()\n",
    "        self._optimizer1.step()\n",
    "        self._optimizer2.zero_grad()\n",
    "        error = self._criterion(self._net2(torch.from_numpy(np.array(features2))), torch.from_numpy(np.array(weights)))\n",
    "        error.backward()\n",
    "        self._optimizer2.step()\n",
    "\n",
    "        if self.replay:\n",
    "            history_buf = list(zip(features1, features2, weights, weights))\n",
    "            self._history.extend(history_buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365867da-1b1d-403f-9ce3-b7b67ed5ad79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pythonosc.dispatcher import Dispatcher\n",
    "from pythonosc.osc_server import ThreadingOSCUDPServer\n",
    "from pythonosc.udp_client import SimpleUDPClient\n",
    "from threading import Thread\n",
    "import time\n",
    "\n",
    "manualMode = True\n",
    "agents = {}\n",
    "\n",
    "agentType = \"joint\"\n",
    "haptic_dims = 6\n",
    "\n",
    "ip = \"127.0.0.1\" # localhost\n",
    "port = 8080\n",
    "destPort = 8081\n",
    "\n",
    "client = SimpleUDPClient(ip, destPort)\n",
    "timings = []\n",
    "\n",
    "def default_handler(address, *args):\n",
    "    print(f\"DEFAULT {address}: {args}\")\n",
    "\n",
    "def auto_switch_handler(address, state, *args):\n",
    "    start_time = time.time()\n",
    "    print(f\"Is Manual {state}\")\n",
    "    manualMode = state\n",
    "    end_time = time.time()\n",
    "    timings.append({\"key\": \"switch\", \"start\": start_time, \"end\": end_time})\n",
    "\n",
    "def manual_set(address, element, *args):\n",
    "    start_time = time.time()\n",
    "    state = args[::3]\n",
    "    low = args[1::3]\n",
    "    high = args[2::3]\n",
    "    agents[element].set_state(state=state, lows=low, highs=high, history=True)\n",
    "    end_time = time.time()\n",
    "    timings.append({\"key\": \"manual_set\", \"start\": start_time, \"end\": end_time})\n",
    "    #print(f\"{element}: {agents[element].state}\")\n",
    "\n",
    "def manual_update(address, element, *args):\n",
    "    start_time = time.time()\n",
    "    state = args[::3]\n",
    "    low = args[1::3]\n",
    "    high = args[2::3]\n",
    "    agents[element].set_state(state=state, lows=low, highs=high, history=False)\n",
    "    end_time = time.time()\n",
    "    timings.append({\"key\": \"manual_update\", \"start\": start_time, \"end\": end_time})\n",
    "\n",
    "def step(address, element):\n",
    "    start_time = time.time()\n",
    "    old_state = agents[element].state\n",
    "    action = agents[element].select_epsilon_greedy_action()\n",
    "    if action is not None:\n",
    "        #print(f\"{element}: Taking action {action}\")\n",
    "        agents[element].apply_action(action)\n",
    "        #print(f\"Transitioned from {old_state} to {agent.state}\")\n",
    "        client.send_message(\"/controller/agentSet\", [element, *agents[element].state])\n",
    "        agents[element].replay_from_history()\n",
    "    else:\n",
    "        print(f\"{element}: All actions excluded! Doing nothing.\")\n",
    "    end_time = time.time()\n",
    "    timings.append({\"key\": \"step\", \"start\": start_time, \"end\": end_time})\n",
    "\n",
    "def reward(address, element, reward, modality=None):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if modality:\n",
    "        \n",
    "        agents[element].process_guiding_reward(reward, modality)\n",
    "    else:\n",
    "        agents[element].process_guiding_reward(reward)\n",
    "    end_time = time.time()\n",
    "    timings.append({\"key\": \"guidance\", \"start\": start_time, \"end\": end_time})\n",
    "    # print(f\"Weights updated from {old_weights} to {agent._weights}\")\n",
    "\n",
    "def zone_reward(address, element, reward):\n",
    "    # Calculate length N_STEPS away on each axis, store in agent\n",
    "    start_time = time.time()\n",
    "    agents[element].process_zone_reward(reward)\n",
    "    end_time = time.time()\n",
    "    timings.append({\"key\": \"zone\", \"start\": start_time, \"end\": end_time})\n",
    "    \n",
    "def activate(address, element, dimension, activation):\n",
    "    print(f\"{element}: Setting dimension {dimension} to {activation}\")\n",
    "    agents[element].update_activation(dimension, activation)\n",
    "    print(f\"{agents[element]._exclude_dims}\")\n",
    "\n",
    "def init(address, element, ndims, step):\n",
    "    if element in agents:\n",
    "        print(f\"Replacing agent {element} with fresh. {ndims} dimensions, initial step {step} (norm)\")\n",
    "    else:\n",
    "        print(f\"New agent {element} with {ndims} dimensions, initial step {step} (norm)\")\n",
    "    #agents[element] = LinearSGDAgent(ndims, step)\n",
    "    if agentType == \"joint\":\n",
    "        agents[element] = NeuralSGDAgent(ndims, step, crediter=UniformCrediter)\n",
    "    elif agentType == \"split\":\n",
    "        agents[element] = SplitNeuralSGDAgent(haptic_dims, ndims - haptic_dims, step, crediter=UniformCrediter)\n",
    "    elif agentType == \"random\":\n",
    "        agents[element] = RandomAgent(ndims, step)\n",
    "\n",
    "def delete(address, element):\n",
    "    if element in agents:\n",
    "        print(f\"Deleting agent {element} ({agents[element]._ndims} dimensions)\")\n",
    "        del agents[element]\n",
    "    else:\n",
    "        print(f\"No agent with identifier {element}!\")\n",
    "\n",
    "dispatcher = Dispatcher()\n",
    "dispatcher.set_default_handler(default_handler)\n",
    "dispatcher.map(\"/uistate/setAutonomous\", auto_switch_handler)\n",
    "dispatcher.map(\"/controller/manualSet\", manual_set)\n",
    "dispatcher.map(\"/controller/updateManual\", manual_update)\n",
    "dispatcher.map(\"/controller/step\", step)\n",
    "dispatcher.map(\"/controller/reward\", reward)\n",
    "dispatcher.map(\"/controller/activate\", activate)\n",
    "dispatcher.map(\"/controller/init\", init)\n",
    "dispatcher.map(\"/controller/zone_reward\", zone_reward)\n",
    "\n",
    "ip = \"127.0.0.1\" # localhost\n",
    "port = 8080\n",
    "\n",
    "with ThreadingOSCUDPServer((ip, port), dispatcher) as server:\n",
    "    def quit_func(address, *args):\n",
    "        print(\"Quit!\")\n",
    "        server.shutdown()\n",
    "        server.server_close()\n",
    "    dispatcher.map(\"/quit\", quit_func)\n",
    "    thread = Thread(target=server.serve_forever)\n",
    "    thread.start()\n",
    "    thread.join()\n",
    "print(\"And we're out!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
