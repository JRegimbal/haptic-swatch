{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a94145-25e8-4c1e-aa84-47ffe81a8c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import sys\n",
    "import torch\n",
    "from collections import deque\n",
    "from functools import reduce\n",
    "\n",
    "print(\"===VERSIONS===\")\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"numpy: {np.__version__}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Scipy: {scipy.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02c4b40-27e5-47de-a609-16cbc33f2eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAM_EPSILON = 0\n",
    "PARAM_ALPHA = 0.02\n",
    "PARAM_GAMMA = 0.5\n",
    "PARAM_BETA = 1\n",
    "PARAM_C = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5480add-9368-49a5-ab53-98b6ebfd9f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizedTiling:\n",
    "    def __init__(self, ndim, ksteps, offset):\n",
    "        self.ndim = ndim\n",
    "        self.k = ksteps\n",
    "        self.offset = offset\n",
    "        assert self.offset < 0, \"Offset must be less than 0\"\n",
    "        assert self.offset >= -1 / self.k, f\"Offset cannot be above {-1/self.k}\"\n",
    "        self.counts = np.zeros(np.power(self.k + 1, self.ndim))\n",
    "\n",
    "    def tile_index(self, state):\n",
    "        assert len(state) == self.ndim, f\"Expected state of dimension {self.ndim}, received {len(state)}\"\n",
    "        shift = (state - self.offset * np.ones(self.ndim)) * self.k\n",
    "        return sum([int(np.power(self.k + 1, j) * (np.floor(shift[j]) if shift[j] < self.k else self.k)) for j in range(self.ndim)])\n",
    "\n",
    "class TilingDensity:\n",
    "    def __init__(self, ndim, ntiles, ksteps):\n",
    "        self.tiles = [NormalizedTiling(ndim, ksteps, -(i + 1)/(ksteps * ntiles)) for i in range(ntiles)]\n",
    "        self.total_count = 0\n",
    "\n",
    "    def count(self, x):\n",
    "        for tiling in self.tiles:\n",
    "            idx = tiling.tile_index(x)\n",
    "            tiling.counts[idx] += 1\n",
    "        self.total_count += 1\n",
    "\n",
    "    def density(self, x):\n",
    "        return sum([tiling.counts[tiling.tile_index(x)] for tiling in self.tiles]) / self.total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c4eb74-b2bc-4bfb-ab48-7e730f5eeaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BasicNN(torch.nn.Module):\n",
    "    def __init__(self, ndim):\n",
    "        super(BasicNN, self).__init__()\n",
    "        hls = round(8*ndim / 3) # chosen by vibes\n",
    "        self.fc1 = torch.nn.Linear(2*ndim, hls)\n",
    "        self.fc2 = torch.nn.Linear(hls, hls)\n",
    "        self.fc3 = torch.nn.Linear(hls, 1)\n",
    "\n",
    "        torch.nn.init.normal_(self.fc1.weight, std=0.1)\n",
    "        torch.nn.init.normal_(self.fc1.weight, std=0.1)\n",
    "        torch.nn.init.normal_(self.fc3.weight, std=0.1)\n",
    "\n",
    "        #torch.nn.init.zeros_(self.fc1.bias)\n",
    "        #torch.nn.init.zeros_(self.fc2.bias)\n",
    "        #torch.nn.init.zeros_(self.fc3.bias)\n",
    "\n",
    "        self.double()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0581cc07-8b06-4f9a-88cb-201da9d0f52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idenfeatures(states, actions):\n",
    "    return np.array([*states, *actions])\n",
    "\n",
    "get_features = idenfeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e6ddc5-91f3-47cc-88b9-88e7615bbb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from scipy.stats import gamma\n",
    "\n",
    "\n",
    "class LinearGammaCrediter:\n",
    "    def __init__(self, ndims):\n",
    "        self._history = []\n",
    "        # From TAMER\n",
    "        self.k = 2.0\n",
    "        self.theta = 0.28\n",
    "        self.delay = 0.20 # seconds\n",
    "\n",
    "    def add_index(self, feature_vec):\n",
    "        self._history.append((feature_vec, time.time()))\n",
    "\n",
    "    def credit(self):\n",
    "        # Prune old times\n",
    "        self._history = [x for x in self._history if time.time() - x[1] < gamma.ppf(0.999, self.k, self.delay, self.theta)]\n",
    "        self._history.sort(key=lambda x: x[1], reverse=True)\n",
    "        if len(self._history) == 0:\n",
    "            raise Exception(\"Empty history array - cannot assign credit\")\n",
    "        # Calculate from remaining\n",
    "        return sum([\n",
    "            (gamma.cdf(x[1], self.k, self.delay, self.theta) - \\\n",
    "             (0 if idx == 0 else gamma.cdf(self._history[idx-1][1], self.k, self.delay, self.theta))) * \\\n",
    "            x[0] for idx, x in enumerate(self._history)\n",
    "        ])\n",
    "\n",
    "class GammaCrediter:\n",
    "    \"\"\"\n",
    "    Unlike the above/older version of the crediter, this version\n",
    "    does not combine the history into one feature vector. Instead,\n",
    "    each vector is returned alongside a weight. Weights sum to 1.\n",
    "    \"\"\"\n",
    "    def __init__(self, ndims):\n",
    "        self._history = []\n",
    "        self.k = 2.0\n",
    "        self.theta = 0.28\n",
    "        self.delay = 0.20\n",
    "        \n",
    "    def add_index(self, feature_vec):\n",
    "        self._history.append((feature_vec, time.time()))\n",
    "    \n",
    "    def credit(self):\n",
    "        # Prune old times\n",
    "        self._history = [x for x in self._history if time.time() - x[1] < gamma.ppf(0.999, self.k, self.delay, self.theta)]\n",
    "        self._history.sort(key=lambda x: x[1], reverse=True)\n",
    "        if len(self._history) == 0:\n",
    "            raise Exception(\"Empty history array - cannot assign credit\")\n",
    "        current_time = time.time()\n",
    "        weights = np.array([gamma.cdf(current_time - x[1], self.k, self.delay, self.theta) - \\\n",
    "                  (0 if idx == 0 else gamma.cdf(current_time - self._history[idx-1][1], self.k, self.delay, self.theta)) \\\n",
    "                  for idx, x in enumerate(self._history)]).reshape((len(self._history), 1))\n",
    "        return (np.array([x[0] for x in self._history]), weights)\n",
    "\n",
    "class UniformCrediter:\n",
    "    \"\"\"\n",
    "    Equally splits reward over interval of t-0.2 to t-4. Weights sum to 1\n",
    "    \"\"\"\n",
    "    def __init__(self, ndims):\n",
    "        self._history = []\n",
    "        self._low = 0.2\n",
    "        self._high = 4.0\n",
    "\n",
    "    def add_index(self, feature_vec):\n",
    "        self._history.append((feature_vec, time.time()))\n",
    "\n",
    "    def credit(self):\n",
    "        # Prune old times\n",
    "        call_time = time.time()\n",
    "        self._history = [x for x in self._history if call_time - x[1] <= self._high]\n",
    "        tmp = [x for x in self._history if call_time - x[1] >= self._low]\n",
    "        if len(tmp) == 0:\n",
    "            raise Exception(\"No eligible history - cannot assign credit\")\n",
    "        return (np.array([x[0] for x in tmp]), np.ones((len(tmp), 1)) / len(tmp)) \n",
    "\n",
    "    def credit2(self):\n",
    "        call_time = time.time()\n",
    "        self._history = [x for x in self._history if call_time - x[1] <= self._high] # Limit to those eligible to receive credit\n",
    "        values = []\n",
    "        next = []\n",
    "        for i in range(len(self._history)):\n",
    "            if call_time - self._history[i][1] >= self._low:\n",
    "                values.append(self._history[i][0])\n",
    "                if i < len(self._history) - 1:\n",
    "                    next.append(self._history[i + 1][0])\n",
    "                else:\n",
    "                    # This case shouldn't happen\n",
    "                    print(\"Entire history eligible for credit, this shouldn't happen in practice!\")\n",
    "                    next.append(0) # Dummy value\n",
    "            else:\n",
    "                break\n",
    "        assert len(values) == len(next), f\"{len(values)} values eligible, but {len(next)} next state-actions found\"\n",
    "        if len(values) == 0:\n",
    "            raise Exception(\"No eligible history - cannot assign credit\")\n",
    "        weights = np.ones((len(values), 1)) / len(values)\n",
    "        return (np.array(values), weights, np.array(next))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8207cf37-3718-4a29-b84e-ee330e00d0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def and_op(x, y):\n",
    "    return x and y\n",
    "\n",
    "# Scurto used alpha = 0.002\n",
    "class NeuralSGDAgent:\n",
    "    def __init__(self, ndims, step, epsilon=PARAM_EPSILON, alpha=PARAM_ALPHA, crediter=UniformCrediter, gamma=PARAM_GAMMA):\n",
    "        self._ndims = ndims\n",
    "        self._step = step\n",
    "        self.crediter = crediter(self._ndims)\n",
    "        self.state = np.zeros(self._ndims)\n",
    "        self.state_lows = np.zeros(self._ndims)\n",
    "        self.state_highs = np.ones(self._ndims)\n",
    "        self._net = BasicNN(self._ndims)\n",
    "        self._criterion = torch.nn.MSELoss()\n",
    "        self._optimizer = torch.optim.SGD(self._net.parameters(), lr=alpha)\n",
    "        a = np.eye(self._ndims) * self._step\n",
    "        self._actions = np.concatenate((a, -a))\n",
    "        self._exclude_dims = set()\n",
    "        self._rng = np.random.default_rng()\n",
    "        self._epsilon = epsilon\n",
    "        self._alpha = alpha # taken from scurto et al 2021\n",
    "        self._beta = PARAM_BETA\n",
    "        self._c = PARAM_C\n",
    "        self._gamma = gamma\n",
    "        n_tiles = int(2 + np.ceil(np.log2(self._ndims)))\n",
    "        k_tile = int(np.ceil(1 / (4 * self._step)))\n",
    "        #n_tiles = int(2 + np.ceil(np.log2(self._ndims)))\n",
    "        #k_tile = int(np.ceil(1/(2 * self._step)))\n",
    "        self.tiling = TilingDensity(self._ndims, n_tiles, k_tile)\n",
    "\n",
    "    def set_state(self, state, *, lows=None, highs=None, action=None, history=True):\n",
    "        # Check range and update\n",
    "        if lows is not None:\n",
    "            assert len(lows) == self._ndims, f\"Expected lows to contain {self._ndims} elements, not {len(lows)}\"\n",
    "            assert reduce(and_op, [x >= 0 for x in lows], True) and reduce(and_op, [x <= 1 for x in lows], True), f\"Low range not normalized: {lows}\"\n",
    "            self.state_lows = lows\n",
    "        if highs is not None:\n",
    "            assert len(highs) == self._ndims, f\"Expected highs to contain {self._ndims} elements, not {len(highs)}\"\n",
    "            assert reduce(and_op, [x >= 0 for x in highs], True) and reduce(and_op, [x <= 1 for x in highs], True), f\"High range not normalized: {highs}\"\n",
    "            self.state_highs = highs\n",
    "        assert reduce(and_op, [x >= 0 for x in state], True) and reduce(and_op, [x <= 1 for x in state], True), f\"State out of bounds {state}\"\n",
    "        if history:\n",
    "            self.tiling.count(self.state)\n",
    "            if action is not None:\n",
    "                self.crediter.add_index(get_features(self.state, action))\n",
    "        self.state = state\n",
    "\n",
    "    def check_bounds(self, state):\n",
    "        return ((state >= 0) & (state <= 1) & (state >= np.array(self.state_lows)) & (state <= np.array(self.state_highs))).all(0)\n",
    "\n",
    "    def get_value(self, state, action):\n",
    "        return self._net(torch.from_numpy(get_features(state, action))).item()\n",
    "\n",
    "    def select_action(self):\n",
    "        max_actions = []\n",
    "        invs = []\n",
    "        max_value = np.NINF\n",
    "        for action in self.included_actions():\n",
    "            next_state = self.state + action\n",
    "            if self.check_bounds(next_state):\n",
    "                reward_value = self.get_value(next_state, action)\n",
    "                explore_value = self._beta * np.power(self.tiling.density(next_state) * self.tiling.total_count + self._c, -0.5)\n",
    "                value = reward_value + explore_value\n",
    "                if np.isclose(max_value, value):\n",
    "                    max_actions.append(action)\n",
    "                elif value > max_value:\n",
    "                    max_value = value\n",
    "                    max_actions = [action]\n",
    "            else:\n",
    "                invs.append(action)\n",
    "        if len(max_actions) > 0:\n",
    "            return max_actions[self._rng.integers(len(max_actions))]\n",
    "        else:\n",
    "            print(\"No valid actions!\")\n",
    "            return None\n",
    "\n",
    "    def select_epsilon_greedy_action(self):\n",
    "        if self._rng.random() < self._epsilon:   \n",
    "            # Random action\n",
    "            invalid = True\n",
    "            actions = self.included_actions()\n",
    "            if len(actions) > 0:\n",
    "                while invalid:\n",
    "                    action = actions[self._rng.integers(len(actions))]\n",
    "                    next_state = self.state + action\n",
    "                    invalid = not self.check_bounds(next_state)\n",
    "                print(f\"Taking random action {action}\")\n",
    "                return action\n",
    "            else:\n",
    "                print(\"No valid actions!\")\n",
    "                return None\n",
    "        else:\n",
    "            return self.select_action() \n",
    "\n",
    "    def apply_action(self, action):\n",
    "        next_state = self.state + action\n",
    "        if self.check_bounds(next_state):\n",
    "            self.set_state(next_state, action=action)\n",
    "        else:\n",
    "            raise Exception(f\"Tried to transition to an invalid state {next_state}.\")\n",
    "\n",
    "    def process_guiding_reward(self, reward):\n",
    "        try:\n",
    "            credit_x, credit_y, next = self.crediter.credit2()\n",
    "            print(credit_x.shape)\n",
    "            credit_y = credit_y * reward # TODO include discounted approximation from next state\n",
    "            # credit_x := credit_x + gamma * q(snext, anext, weights)\n",
    "            if self._gamma > 0:\n",
    "                start = time.time()\n",
    "                credit_x = credit_x + self._gamma * self._net(torch.from_numpy(next)).detach().numpy()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: {e}\")\n",
    "            print(\"Not applying reward...\")\n",
    "            return\n",
    "        print(\"determined guidance\")\n",
    "        self._optimizer.zero_grad()\n",
    "        error = self._criterion(self._net(torch.from_numpy(credit_x)), torch.from_numpy(credit_y))\n",
    "        print(f\"Error: {error}\")\n",
    "        error.backward()\n",
    "        self._optimizer.step()\n",
    "        print(\"updated model\")\n",
    "\n",
    "    def process_zone_reward(self, reward):\n",
    "        # Positive reward - apply towards this point - negative reward - apply away from this point\n",
    "        # Directions include those disabled so we properly encode the zone here\n",
    "        SCALE_FACTOR = 1\n",
    "        ZONE_STEPS = 3 # Arbitrarily chosen, TODO scale to match number of divisions in a dimension. Note that Scurto et al. effectively used 5.\n",
    "        features = []\n",
    "        weights = []\n",
    "        for action in self._actions:\n",
    "            state = self.state\n",
    "            # Iterate through steps and apply 1) reward in direction we want to move 2) -reward in direction we do not want to move\n",
    "            for _step in range(1, ZONE_STEPS + 1):\n",
    "                tmp = state + action\n",
    "                if not self.check_bounds(tmp):\n",
    "                    break\n",
    "                features.append(get_features(tmp, action))\n",
    "                weights.append(np.array([-float(SCALE_FACTOR * reward)]))\n",
    "                features.append(get_features(state, -action))\n",
    "                weights.append(np.array([float(SCALE_FACTOR * reward)]))\n",
    "                \n",
    "                state = tmp\n",
    "        self._optimizer.zero_grad()\n",
    "        error = self._criterion(self._net(torch.from_numpy(np.array(features))), torch.from_numpy(np.array(weights)))\n",
    "        print(f\"Error: {error}\")\n",
    "        error.backward()\n",
    "        self._optimizer.step()\n",
    "        \n",
    "    def update_activation(self, dimension, activation):\n",
    "        if activation:\n",
    "            self._exclude_dims.discard(dimension)\n",
    "        else:\n",
    "            self._exclude_dims.add(dimension)\n",
    "\n",
    "    def included_actions(self):\n",
    "        # Set of actions that do not modify the 0-indexed dimensions in self._exclude_dims\n",
    "        return np.array([act for act in self._actions if reduce(lambda x, y: x and y, [act[dim] == 0 for dim in self._exclude_dims], True)])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e41260c-3066-40d2-8951-c65a8b1042d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def and_op(x, y):\n",
    "    return x and y\n",
    "\n",
    "# Scurto used alpha = 0.002\n",
    "class SplitNeuralSGDAgent:\n",
    "    def __init__(self, ndims1, ndims2, step, epsilon=PARAM_EPSILON, alpha=PARAM_ALPHA, crediter=UniformCrediter, gamma=PARAM_GAMMA):\n",
    "        self._ndims = ndims1 + ndims2\n",
    "        self._split_index = ndims1\n",
    "        self._step = step\n",
    "        self.crediter1 = crediter(ndims1)\n",
    "        self.crediter2 = crediter(ndims2)\n",
    "        self.state = np.zeros(self._ndims)\n",
    "        self.state_lows = np.zeros(self._ndims)\n",
    "        self.state_highs = np.ones(self._ndims)\n",
    "        self._net1 = BasicNN(ndims1)\n",
    "        self._net2 = BasicNN(ndims2)\n",
    "        self._criterion = torch.nn.MSELoss()\n",
    "        self._optimizer1 = torch.optim.SGD(self._net1.parameters(), lr=alpha)\n",
    "        self._optimizer2 = torch.optim.SGD(self._net2.parameters(), lr=alpha)\n",
    "        a = np.eye(self._ndims) * self._step\n",
    "        self._actions = np.concatenate((a, -a))\n",
    "        self._exclude_dims = set()\n",
    "        self._rng = np.random.default_rng()\n",
    "        self._epsilon = epsilon\n",
    "        self._alpha = alpha # taken from scurto et al 2021\n",
    "        self._beta = PARAM_BETA\n",
    "        self._c = PARAM_C\n",
    "        self._gamma = gamma\n",
    "        n_tiles = int(2 + np.ceil(np.log2(self._ndims)))\n",
    "        k_tile = int(np.ceil(1 / (4 * self._step)))\n",
    "        #n_tiles = int(2 + np.ceil(np.log2(self._ndims)))\n",
    "        #k_tile = int(np.ceil(1/(2 * self._step)))\n",
    "        self.tiling = TilingDensity(self._ndims, n_tiles, k_tile)\n",
    "\n",
    "    def split(self, value):\n",
    "        assert len(value) == self._ndims, f\"Expected vector of size {self._ndims}, received {len(value)}\"\n",
    "        return (value[:self._split_index], value[self._split_index:])\n",
    "\n",
    "    def set_state(self, state, *, lows=None, highs=None, action=None, history=True):\n",
    "        # Check range and update\n",
    "        if lows is not None:\n",
    "            assert len(lows) == self._ndims, f\"Expected lows to contain {self._ndims} elements, not {len(lows)}\"\n",
    "            assert reduce(and_op, [x >= 0 for x in lows], True) and reduce(and_op, [x <= 1 for x in lows], True), f\"Low range not normalized: {lows}\"\n",
    "            self.state_lows = lows\n",
    "        if highs is not None:\n",
    "            assert len(highs) == self._ndims, f\"Expected highs to contain {self._ndims} elements, not {len(highs)}\"\n",
    "            assert reduce(and_op, [x >= 0 for x in highs], True) and reduce(and_op, [x <= 1 for x in highs], True), f\"High range not normalized: {highs}\"\n",
    "            self.state_highs = highs\n",
    "        assert reduce(and_op, [x >= 0 for x in state], True) and reduce(and_op, [x <= 1 for x in state], True), f\"State out of bounds {state}\"\n",
    "        if history:\n",
    "            self.tiling.count(self.state)\n",
    "            if action is not None:\n",
    "                state1, state2 = self.split(self.state)\n",
    "                action1, action2 = self.split(action)\n",
    "                self.crediter1.add_index(get_features(state1, action1))\n",
    "                self.crediter2.add_index(get_features(state2, action2))\n",
    "        self.state = state\n",
    "\n",
    "    def check_bounds(self, state):\n",
    "        return ((state >= 0) & (state <= 1) & (state >= np.array(self.state_lows)) & (state <= np.array(self.state_highs))).all(0)\n",
    "\n",
    "    def get_value(self, state, action):\n",
    "        state1, state2 = self.split(state)\n",
    "        action1, action2 = self.split(action)\n",
    "        return self._net1(torch.from_numpy(get_features(state1, action1))).item() + self._net2(torch.from_numpy(get_features(state2, action2))).item()\n",
    "\n",
    "    def select_action(self):\n",
    "        max_actions = []\n",
    "        invs = []\n",
    "        max_value = np.NINF\n",
    "        for action in self.included_actions():\n",
    "            next_state = self.state + action\n",
    "            if self.check_bounds(next_state):\n",
    "                reward_value = self.get_value(next_state, action)\n",
    "                explore_value = self._beta * np.power(self.tiling.density(next_state) * self.tiling.total_count + self._c, -0.5)\n",
    "                value = reward_value + explore_value\n",
    "                if np.isclose(max_value, value):\n",
    "                    max_actions.append(action)\n",
    "                elif value > max_value:\n",
    "                    max_value = value\n",
    "                    max_actions = [action]\n",
    "            else:\n",
    "                invs.append(action)\n",
    "        if len(max_actions) > 0:\n",
    "            return max_actions[self._rng.integers(len(max_actions))]\n",
    "        else:\n",
    "            print(\"No valid actions!\")\n",
    "            return None\n",
    "\n",
    "    def select_epsilon_greedy_action(self):\n",
    "        if self._rng.random() < self._epsilon:   \n",
    "            # Random action\n",
    "            invalid = True\n",
    "            actions = self.included_actions()\n",
    "            if len(actions) > 0:\n",
    "                while invalid:\n",
    "                    action = actions[self._rng.integers(len(actions))]\n",
    "                    next_state = self.state + action\n",
    "                    invalid = not self.check_bounds(next_state)\n",
    "                print(f\"Taking random action {action}\")\n",
    "                return action\n",
    "            else:\n",
    "                print(\"No valid actions!\")\n",
    "                return None\n",
    "        else:\n",
    "            return self.select_action() \n",
    "\n",
    "    def apply_action(self, action):\n",
    "        next_state = self.state + action\n",
    "        if self.check_bounds(next_state):\n",
    "            self.set_state(next_state, action=action)\n",
    "        else:\n",
    "            raise Exception(f\"Tried to transition to an invalid state {next_state}.\")\n",
    "\n",
    "    def process_guiding_reward(self, reward):\n",
    "        try:\n",
    "            credit_x1, credit_y1, next1 = self.crediter1.credit2()\n",
    "            credit_x2, credit_y2, next2 = self.crediter2.credit2()\n",
    "            credit_y1 = credit_y1 * reward\n",
    "            credit_y2 = credit_y2 * reward\n",
    "            # credit_x := credit_x + gamma * q(snext, anext, weights)\n",
    "            if self._gamma > 0:\n",
    "                start = time.time()\n",
    "                credit_x1 = credit_x1 + self._gamma * self._net1(torch.from_numpy(next1)).detach().numpy()\n",
    "                credit_x2 = credit_x2 + self._gamma * self._net2(torch.from_numpy(next2)).detach().numpy()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: {e}\")\n",
    "            print(\"Not applying reward...\")\n",
    "            return\n",
    "        self._optimizer1.zero_grad()\n",
    "        error = self._criterion(self._net1(torch.from_numpy(credit_x1)), torch.from_numpy(credit_y1))\n",
    "        print(f\"Error: {error}\")\n",
    "        error.backward()\n",
    "        self._optimizer1.step()\n",
    "        self._optimizer2.zero_grad()\n",
    "        error = self._criterion(self._net2(torch.from_numpy(credit_x2)), torch.from_numpy(credit_y2))\n",
    "        print(f\"Error: {error}\")\n",
    "        error.backward()\n",
    "        self._optimizer2.step()\n",
    "\n",
    "    def process_zone_reward(self, reward):\n",
    "        # Positive reward - apply towards this point - negative reward - apply away from this point\n",
    "        # Directions include those disabled so we properly encode the zone here\n",
    "        SCALE_FACTOR = 1\n",
    "        ZONE_STEPS = 3 # Arbitrarily chosen, TODO scale to match number of divisions in a dimension. Note that Scurto et al. effectively used 5.\n",
    "        features1 = []\n",
    "        features2 = []\n",
    "        weights = []\n",
    "        for action in self._actions:\n",
    "            state = self.state\n",
    "            # Iterate through steps and apply 1) reward in direction we want to move 2) -reward in direction we do not want to move\n",
    "            for _step in range(1, ZONE_STEPS + 1):\n",
    "                tmp = state + action\n",
    "                if not self.check_bounds(tmp):\n",
    "                    break\n",
    "                tmp1, tmp2 = self.split(tmp)\n",
    "                state1, state2 = self.split(state)\n",
    "                action1, action2 = self.split(action)\n",
    "                features1.append(get_features(tmp1, action1))\n",
    "                features2.append(get_features(tmp2, action2))\n",
    "                weights.append(np.array([-float(SCALE_FACTOR * reward)]))\n",
    "                features1.append(get_features(state1, -action1))\n",
    "                features2.append(get_features(state2, -action2))\n",
    "                weights.append(np.array([float(SCALE_FACTOR * reward)]))\n",
    "                \n",
    "                state = tmp\n",
    "        self._optimizer1.zero_grad()\n",
    "        error = self._criterion(self._net1(torch.from_numpy(np.array(features1))), torch.from_numpy(np.array(weights)))\n",
    "        print(f\"Error: {error}\")\n",
    "        error.backward()\n",
    "        self._optimizer1.step()\n",
    "        self._optimizer2.zero_grad()\n",
    "        error = self._criterion(self._net2(torch.from_numpy(np.array(features2))), torch.from_numpy(np.array(weights)))\n",
    "        error.backward()\n",
    "        self._optimizer2.step()\n",
    "        \n",
    "    def update_activation(self, dimension, activation):\n",
    "        if activation:\n",
    "            self._exclude_dims.discard(dimension)\n",
    "        else:\n",
    "            self._exclude_dims.add(dimension)\n",
    "\n",
    "    def included_actions(self):\n",
    "        # Set of actions that do not modify the 0-indexed dimensions in self._exclude_dims\n",
    "        return np.array([act for act in self._actions if reduce(lambda x, y: x and y, [act[dim] == 0 for dim in self._exclude_dims], True)])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb88bc5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pythonosc.dispatcher import Dispatcher\n",
    "from pythonosc.osc_server import ThreadingOSCUDPServer\n",
    "from pythonosc.udp_client import SimpleUDPClient\n",
    "from threading import Thread\n",
    "import time\n",
    "\n",
    "manualMode = True\n",
    "agents = {}\n",
    "\n",
    "agentType = \"split\"\n",
    "haptic_dims = 6\n",
    "\n",
    "ip = \"127.0.0.1\" # localhost\n",
    "port = 8080\n",
    "destPort = 8081\n",
    "\n",
    "client = SimpleUDPClient(ip, destPort)\n",
    "\n",
    "def default_handler(address, *args):\n",
    "    print(f\"DEFAULT {address}: {args}\")\n",
    "\n",
    "def auto_switch_handler(address, state, *args):\n",
    "    print(f\"Is Manual {state}\")\n",
    "    manualMode = state\n",
    "\n",
    "def manual_set(address, element, *args):\n",
    "    state = args[::3]\n",
    "    low = args[1::3]\n",
    "    high = args[2::3]\n",
    "    agents[element].set_state(state=state, lows=low, highs=high, history=True)\n",
    "    #print(f\"{element}: {agents[element].state}\")\n",
    "\n",
    "def manual_update(address, element, *args):\n",
    "    state = args[::3]\n",
    "    low = args[1::3]\n",
    "    high = args[2::3]\n",
    "    agents[element].set_state(state=state, lows=low, highs=high, history=False)\n",
    "\n",
    "def step(address, element):\n",
    "    old_state = agents[element].state\n",
    "    action = agents[element].select_epsilon_greedy_action()\n",
    "    if action is not None:\n",
    "        #print(f\"{element}: Taking action {action}\")\n",
    "        agents[element].apply_action(action)\n",
    "        #print(f\"Transitioned from {old_state} to {agent.state}\")\n",
    "        client.send_message(\"/controller/agentSet\", [element, *agents[element].state])\n",
    "    else:\n",
    "        print(f\"{element}: All actions excluded! Doing nothing.\")\n",
    "\n",
    "def reward(address, element, reward):\n",
    "    agents[element].process_guiding_reward(reward)\n",
    "    # print(f\"Weights updated from {old_weights} to {agent._weights}\")\n",
    "\n",
    "def zone_reward(address, element, reward):\n",
    "    # Calculate length N_STEPS away on each axis, store in agent\n",
    "    agents[element].process_zone_reward(reward)\n",
    "    \n",
    "def activate(address, element, dimension, activation):\n",
    "    print(f\"{element}: Setting dimension {dimension} to {activation}\")\n",
    "    agents[element].update_activation(dimension, activation)\n",
    "    print(f\"{agents[element]._exclude_dims}\")\n",
    "\n",
    "def init(address, element, ndims, step):\n",
    "    if element in agents:\n",
    "        print(f\"Replacing agent {element} with fresh. {ndims} dimensions, initial step {step} (norm)\")\n",
    "    else:\n",
    "        print(f\"New agent {element} with {ndims} dimensions, initial step {step} (norm)\")\n",
    "    #agents[element] = LinearSGDAgent(ndims, step)\n",
    "    if agentType == \"joint\":\n",
    "        agents[element] = NeuralSGDAgent(ndims, step, crediter=UniformCrediter)\n",
    "    elif agentType == \"split\":\n",
    "        agents[element] = SplitNeuralSGDAgent(haptic_dims, ndims - haptic_dims, step, crediter=UniformCrediter)\n",
    "\n",
    "def delete(address, element):\n",
    "    if element in agents:\n",
    "        print(f\"Deleting agent {element} ({agents[element]._ndims} dimensions)\")\n",
    "        del agents[element]\n",
    "    else:\n",
    "        print(f\"No agent with identifier {element}!\")\n",
    "\n",
    "dispatcher = Dispatcher()\n",
    "dispatcher.set_default_handler(default_handler)\n",
    "dispatcher.map(\"/uistate/setAutonomous\", auto_switch_handler)\n",
    "dispatcher.map(\"/controller/manualSet\", manual_set)\n",
    "dispatcher.map(\"/controller/updateManual\", manual_update)\n",
    "dispatcher.map(\"/controller/step\", step)\n",
    "dispatcher.map(\"/controller/reward\", reward)\n",
    "dispatcher.map(\"/controller/activate\", activate)\n",
    "dispatcher.map(\"/controller/init\", init)\n",
    "dispatcher.map(\"/controller/zone_reward\", zone_reward)\n",
    "\n",
    "ip = \"127.0.0.1\" # localhost\n",
    "port = 8080\n",
    "\n",
    "with ThreadingOSCUDPServer((ip, port), dispatcher) as server:\n",
    "    def quit_func(address, *args):\n",
    "        print(\"Quit!\")\n",
    "        server.shutdown()\n",
    "        server.server_close()\n",
    "    dispatcher.map(\"/quit\", quit_func)\n",
    "    thread = Thread(target=server.serve_forever)\n",
    "    thread.start()\n",
    "    thread.join()\n",
    "print(\"And we're out!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2934e35-f6df-4106-8aff-caf7b31d939e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
